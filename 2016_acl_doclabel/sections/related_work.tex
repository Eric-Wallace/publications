




















Text classification---a ubiquitous machine learning tool for
classifying text~\cite{zhang-10}---requires feature
extraction~\cite{lewis1992feature}. These are both well-trodden areas
of \abr{nlp} research.  The difficulty is often creating the training
data~\cite{hwa2004sample,osborne2004ensemble}; coding theory is an
entire subfield of social science devoted to creating, formulating, and applying labels to text data~\cite{saldana-12,musialek-16}.
Crowdsourcing~\cite{snow-08} and active
learning~\cite{settles2012active}, can decrease the cost of annotation
but only \emph{after} a label set exists.

\nocite{iyyer2014political,anand2011believe,nikolova2011collecting}

\name{} quantitatively shows that corpus overviews aid text
understanding, building on traditional interfaces for gaining both
local and global information~\cite{hearst-96}.  More elaborate
interfaces~\cite{eisenstein-12,chaney-12} provide richer information given a fixed topic model.
Alternatively, because topic models are imperfect~\cite{boyd2014care},
refining underlying topic models may also improve users'
understanding of a corpus~\cite{choo-13,hoque-15}.

Summarizing document collections through discovered topics can happen
through raw topics labeled manually by users~\cite{talley-11}, automatically~\cite{lau-11}, or by learning a mapping
from labels to topics~\cite{ramage-09}.  When there is not a direct
correspondence between topics and labels, classifiers learn a
mapping~\cite{blei-07b,zhu-09,Nguyen:Boyd-Graber:Lund:Seppi:Ringger-2015}.
Because we want topics to be consistent between users, we use a
classifier with static topics in \name{}. \name{} combines active
learning with topic models to provide both global and local knowledge
document labeling requires.







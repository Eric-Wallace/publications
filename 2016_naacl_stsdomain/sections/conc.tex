We present hierarchical Bayesian models for supervised domain
adaptation and multitask learning of short text similarity models. In
our experiments, these models show improved overall performance across
different domains and tasks.  We intend to explore adaptation to
other \sts{} applications and with additional \sts{} features (e.g.,
word and character $n$-gram overlap) in future.  Unsupervised and
semi-supervised domain adaptation techniques that do not assume the
availability of in-domain annotations or that learn effective domains
splits~\cite{Hu:Zhai:Eidelman:Boyd-Graber-2014} provide another avenue
for future research.



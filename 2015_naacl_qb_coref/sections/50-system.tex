\section{Experimental Comparison of Coreference Systems}
\label{sec:system}

We evaluate the widely used Berkeley coreference system~\cite{DurrettKlein2013}
on our dataset to show that models trained on newswire data cannot effectively
resolve coreference in quiz bowl data. Training and evaluating the Berkeley
system on quiz bowl data also results in poor performance.~\footnote{We use
  default options, including hyperparameters tuned on OntoNotes} This result
motivates us to build an end-to-end coreference resolution system that includes
a data-driven mention detector (as opposed to Berkeley's rule-based one) and a
simple pairwise classifier. Using our mentions and only six feature types, we
are able to outperform the Berkeley system on our data. Finally, we explore the
linguistic phenomena that make quiz bowl coreference so hard and draw insights
from our analysis that may help to guide the next generation of coreference
systems.

\subsection{Evaluating the Berkeley System on Quiz Bowl Data}

We use two publicly-available pretrained models supplied with the Berkeley
coreference system, \emph{Surface} and \emph{Final}, which are trained on the
entire OntoNotes dataset. The difference between the two models is that
\emph{Final} includes semantic features. We report results with both models to
see if the extra semantic features in \emph{Final} are expressive enough to
capture quiz bowl's inherently difficult coreferences. We also train the
Berkeley system on quiz bowl data and compare the performance of these models to
the pretrained newswire ones in Table~\ref{tab:berk}. Our results are obtained
by running a five-fold cross-validation on our dataset. The results show that
newswire is a poor source of data for learning how to resolve quiz bowl
coreferences and prompted us to see how well a pairwise classifier does in
comparison. To build an end-to-end coreference system using this classifier, we
first need to know which parts of the text are ``mentions'', or spans of a text
that refer to real world entities. In the next section we talk about our mention
detection system.

\subsection{A Simple Mention Detector}
Detecting mentions is done differently by different coreference systems. The
Berkeley system does rule-based mention detection to detect every NP span, every
pronoun, and every named entity, which leads to many spurious mentions. This
process is based on an earlier work of~\newcite{kummerfeld2011mention}, which
assumes that every maximal projection of a noun or a pronoun is a mention and
uses rules to weed out spurious mentions. Instead of using such a rule-based
mention detector, our system detects mentions via sequence labeling, as
detecting mentions is essentially a problem of detecting start and stop points
in spans of text. We solve this sequence tagging problem using the
\abr{mallet}~\cite{McCallumMALLET} implementation of conditional random
fields~\cite{lafferty2001conditional}. Since our data contain nested mentions,
the sequence labels are \abr{bio} markers~\cite{ratinov2009design}. The features
we use, which are similar to those used in \newcite{kummerfeld2011mention}, are:
\begin{itemize*}
\item the token itself
\item the part of speech
\item the named entity type
\item a dependency relation concatenated with the parent token\footnote{These features were obtained using the Stanford dependency parser~\cite{de2006generating}.}
\end{itemize*}

Using these simple features, we obtain surprisingly good results. When comparing
our detected mentions to gold standard mentions on the quiz bowl dataset using
exact matches, we obtain 76.1\% precision, 69.6\% recall, and 72.7\% $F_1$
measure. Now that we have high-quality mentions, we can feed each pair of
mentions into a pairwise mention classifier.

\begin{table}[t!]
\begin{center}
\begin{tabular}{llccc}
\toprule
& & \multicolumn{3}{c}{\abr{muc}} \\
\cmidrule(r){3-5}
System & Train & P & R & $F_1$\\
\midrule
Surface & OntoN & 47.22 & 27.97 & 35.13  \\
Final & OntoN & 50.79 & 30.77 & 38.32 \\
\midrule
Surface & QB & \bf 60.44 & 31.31 & 41.2 \\
Final & QB & 60.21 & \bf 33.41 & \bf 42.35 \\
\bottomrule
\end{tabular}
\end{center}
\caption{The top half of the table represents Berkeley models trained on
  OntoNotes 4.0 data, while the bottom half shows models trained on quiz bowl
  data. The \abr{muc} $F_1$-score of the Berkeley system on OntoNotes text is
  66.4, which when compared to these results prove that quiz bowl coreference is
  significantly different than OntoNotes coreference.}

\label{tab:berk}
\end{table}

\subsection{A Simple Coref Classifier}
We follow previous pairwise coreference
systems~\cite{ng2002improving,uryupina2006coreference,Versley2008} in extracting
a set of lexical, syntactic, and semantic features from two mentions to
determine whether they are coreferent. For example, if \emph{Sylvia Plath},
\emph{he}, and \emph{she} are all of the mentions that occur in a document, our
classifier gives predictions for the pairs \emph{he}---\emph{Sylvia Plath},
\emph{she}---\emph{Sylvia Plath}, and \emph{he}---\emph{she}.

Given two mentions in a document, $m_1$ and $m_2$, we generate the following
features and feed them to a logistic regression classifier:
\begin{itemize*}
\item binary indicators for all tokens contained in $m_1$ and $m_2$ concatenated
  with their parts-of-speech
\item same as above except for an $n$-word window before and after $m_1$ and $m_2$
\item how many tokens separate $m_1$ and $m_2$
\item how many sentences separate $m_1$ and $m_2$
\item the cosine similarity of \texttt{word2vec}~\cite{mikolov2013efficient} vector representations of $m_1$ and $m_2$; we obtain these vectors by averaging the word embeddings for all words in each mention. We use publicly-available 300-dimensional embeddings that have been pretrained on 100B tokens from Google News.
\item same as above except with publicly-available 300-dimensional \texttt{GloVe}~\cite{glove2014} vector embeddings trained on 840B tokens from the Common Crawl
\end{itemize*}

The first four features are standard in coreference literature and similar to
some of the surface features used by the Berkeley system, while the word
embedding similarity scores increase our F-measure by about 5 points on the quiz
bowl data. Since they have been trained on huge corpora, the word embeddings
allow us to infuse world knowledge into our model; for instance, the vector for
\emph{Russian} is more similar to \emph{Dostoevsky} than \emph{Hemingway}.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{2015_naacl_qb_coref/auto_fig/compare}
  \caption{All models are trained and evaluated on quiz bowl data via five fold
    cross validation on $F_1$, precision, and recall. Berkeley/\abr{crf}/Gold refers
    to the mention detection used, \abr{lr} refers to our logistic regression
    model and \emph{QB Final} refers to the Berkeley model trained on quiz bowl
    data. Our model outperforms the Berkeley model on every metric when using
    our detected \abr{crf} mentions. When given gold mentions, \abr{lr}
    outperforms Berkeley \emph{QB Final} in five of nine metrics.}
  \label{fig:our}
\end{figure*}

Figure~\ref{fig:our} shows that our logistic regression model (\abr{lr})
outperforms the Berkeley system on numerous metrics when trained and evaluated
on the quiz bowl dataset. We use precision, recall, and $F_1$, metrics applied
to \abr{muc}, \abr{bcub}, and \abr{ceafe} measures used for comparing
coreference systems.\footnote{The \abr{muc}~\cite{vilain1995model} score is the
  minimum number of links between mentions to be inserted or deleted when
  mapping the output to a gold standard key set.
  \abr{bcub}~\cite{bagga1998algorithms} computes the precision and recall for
  all mentions separately and then combines them to get the final precision and
  recall of the output. \abr{ceafe}~\cite{luo2005coreference} is an improvement
  on \abr{bcub} and does not use entities multiple times to compute scores.} We
find that our \abr{lr} model outperforms Berkeley by a wide margin when both are
trained on the mentions found by our mention detector (\abr{crf}). For four
metrics, the \abr{crf} mentions actually improve over training on the gold
mentions.

Why does the \abr{lr} model outperform Berkeley when both are trained on our
quiz bowl dataset? We hypothesize that some of Berkeley's features, while
helpful for sparse OntoNotes coreferences, do not offer the same utility in the
denser quiz bowl domain. Compared to newswire text, our dataset contains a much
larger percentage of complex coreference types that require world knowledge to
resolve. Since the Berkeley system lacks semantic features, it is unlikely to
correctly resolve these instances, whereas the pretrained word embedding
features give our \abr{lr} model a better chance of handling them correctly.
Another difference between the two models is that the Berkeley system ranks
mentions as opposed to doing pairwise classification like our \abr{lr} model,
and the mention ranking features may be optimized for newswire text.
























\subsection{Why Quiz Bowl Coreference is Challenging}

While models trained on newswire falter on these data, is this simply a domain
adaptation issue or something deeper?  In the rest of this section, we examine
specific examples to understand why quiz bowl coreference is so difficult. We
begin with examples that \emph{Final} gets wrong.

\begin{quote}
  \emph{This writer} depicted a group of samurai's battle against an imperial. For
  ten points, name \emph{this Japanese writer of A Personal Matter and The
    Silent Cry}.
\end{quote}

While \emph{Final} identifies most of pronouns associated with Kenzaburo Oe (the
answer), it cannot recognize that the theme of the entire paragraph is building
to the final reference, ``this Japanese writer'', despite the many
Japanese-related ideas in the text of the question (e.g., Samurai and
emperor). \emph{Final} also cannot reason effectively about coreferences that
are tied together by similar modifiers as in the below example:
\begin{quote}
  That \emph{title character} plots to secure a ``beautiful death'' for Lovberg
  by burning his manuscript and giving him a pistol. For 10 points, name this
  play in which \emph{the titular wife of George Tesman} commits suicide.
\end{quote}

While a reader can connect ``titular'' and ``title'' to the same character,
Hedda Gabler, the Berkeley system fails to make this inference. These data are a
challenge for all systems, as they require extensive world knowledge.  For
example, in the following sentence, a model must know that the story referenced
in the first sentence is about a dragon and that dragons can fly.
\begin{quote}
  The protagonist of \emph{one of this man's works} erects a sign claiming that
  that story's title figure will fly to heaven from a pond. Identify this author
  of \emph{Dragon: the Old Potter's Tale}
\end{quote}

Humans solve cases like these using a vast amount of external knowledge, but
existing models lack information about worlds (both real and imaginary) and thus
cannot confidently mark these coreferences. We discuss coreference work that
incorporates external resources such as Wikipedia in the next section; our aim
is to provide a dataset that benefits more from this type of information than
newswire does.
